{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c57fc61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install inflect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "597ddba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0908bd58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntokenization\\n\\nConversion from uppercase to lower case\\n\\nnumber removal\\n\\nnumber to text\\n\\ninflect library installation\\n\\nremove punctuation marks\\n\\nremoval whitespace\\n\\nremove stop word\\n\\nstemming and lemmatization\\n\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "tokenization\n",
    "\n",
    "Conversion from uppercase to lower case\n",
    "\n",
    "number removal\n",
    "\n",
    "number to text\n",
    "\n",
    "inflect library installation\n",
    "\n",
    "remove punctuation marks\n",
    "\n",
    "removal whitespace\n",
    "\n",
    "remove stop word\n",
    "\n",
    "stemming and lemmatization\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "82ed47cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries \n",
    "# Tokenization\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer=WordNetLemmatizer()\n",
    "from nltk import pos_tag\n",
    "to_be_removed = set(stopwords.words('english'))\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer(stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a5f4868d",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = \"\"\"1.Her hair was a tangled mess which she tried to make presentable by putting in a lump on the top of her head. 2.It didn't really work although it was a valiant attempt. While most people simply noticed the tangled mess on top of her head, what most people failed to understand that within the tangles mess was an entirely new year.3.That was her secret.4. She kept worlds on top of her head.\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de41618c",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "Tokenization is the process of breaking down the given text in natural language processing into the smallest unit in a sentence called a token. Punctuation marks, words, and numbers can be considered tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b8428af0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1.Her', 'hair', 'was', 'a', 'tangled', 'mess', 'which', 'she', 'tried', 'to', 'make', 'presentable', 'by', 'putting', 'in', 'a', 'lump', 'on', 'the', 'top', 'of', 'her', 'head', '.', '2.It', 'did', \"n't\", 'really', 'work', 'although', 'it', 'was', 'a', 'valiant', 'attempt', '.', 'While', 'most', 'people', 'simply', 'noticed', 'the', 'tangled', 'mess', 'on', 'top', 'of', 'her', 'head', ',', 'what', 'most', 'people', 'failed', 'to', 'understand', 'that', 'within', 'the', 'tangles', 'mess', 'was', 'an', 'entirely', 'new', 'year.3.That', 'was', 'her', 'secret.4', '.', 'She', 'kept', 'worlds', 'on', 'top', 'of', 'her', 'head', '.']\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "tokenized_text=word_tokenize(txt)\n",
    "print(tokenized_text)\n",
    "print(type(tokenized_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "47226afe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1.Her hair was a tangled mess which she tried to make presentable by putting in a lump on the top of her head.',\n",
       " \"2.It didn't really work although it was a valiant attempt.\",\n",
       " 'While most people simply noticed the tangled mess on top of her head, what most people failed to understand that within the tangles mess was an entirely new year.3.That was her secret.4.',\n",
       " 'She kept worlds on top of her head.']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split the text into sentences\n",
    "sent_tokens = sent_tokenize(txt)\n",
    "sent_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "22e38d5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1.Her',\n",
       " 'hair',\n",
       " 'was',\n",
       " 'a',\n",
       " 'tangled',\n",
       " 'mess',\n",
       " 'which',\n",
       " 'she',\n",
       " 'tried',\n",
       " 'to',\n",
       " 'make',\n",
       " 'presentable',\n",
       " 'by',\n",
       " 'putting',\n",
       " 'in',\n",
       " 'a',\n",
       " 'lump',\n",
       " 'on',\n",
       " 'the',\n",
       " 'top',\n",
       " 'of',\n",
       " 'her',\n",
       " 'head',\n",
       " '.',\n",
       " '2.It',\n",
       " 'did',\n",
       " \"n't\",\n",
       " 'really',\n",
       " 'work',\n",
       " 'although',\n",
       " 'it',\n",
       " 'was',\n",
       " 'a',\n",
       " 'valiant',\n",
       " 'attempt',\n",
       " '.',\n",
       " 'While',\n",
       " 'most',\n",
       " 'people',\n",
       " 'simply',\n",
       " 'noticed',\n",
       " 'the',\n",
       " 'tangled',\n",
       " 'mess',\n",
       " 'on',\n",
       " 'top',\n",
       " 'of',\n",
       " 'her',\n",
       " 'head',\n",
       " ',',\n",
       " 'what',\n",
       " 'most',\n",
       " 'people',\n",
       " 'failed',\n",
       " 'to',\n",
       " 'understand',\n",
       " 'that',\n",
       " 'within',\n",
       " 'the',\n",
       " 'tangles',\n",
       " 'mess',\n",
       " 'was',\n",
       " 'an',\n",
       " 'entirely',\n",
       " 'new',\n",
       " 'year.3.That',\n",
       " 'was',\n",
       " 'her',\n",
       " 'secret.4',\n",
       " '.',\n",
       " 'She',\n",
       " 'kept',\n",
       " 'worlds',\n",
       " 'on',\n",
       " 'top',\n",
       " 'of',\n",
       " 'her',\n",
       " 'head',\n",
       " '.']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split the text into words\n",
    "word_tokens = word_tokenize(txt)\n",
    "word_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1fbd243c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1.Her', 'hair', 'tangled', 'mess', 'tried', 'make', 'presentable', 'putting', 'lump', 'top', 'head', '.', '2.It', \"n't\", 'really', 'work', 'although', 'valiant', 'attempt', '.', 'While', 'people', 'simply', 'noticed', 'tangled', 'mess', 'top', 'head', ',', 'people', 'failed', 'understand', 'within', 'tangles', 'mess', 'entirely', 'new', 'year.3.That', 'secret.4', '.', 'She', 'kept', 'worlds', 'top', 'head', '.']\n"
     ]
    }
   ],
   "source": [
    "modified_token_list=[word for word in tokenized_text if not word in to_be_removed]\n",
    "print(modified_token_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a216d70c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final string :  .Her hair was a tangled mess which she tried to make presentable by putting in a lump on the top of her head. .It didn't really work although it was a valiant attempt. While most people simply noticed the tangled mess on top of her head, what most people failed to understand that within the tangles mess was an entirely new year..That was her secret.. She kept worlds on top of her head.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ini_string = txt = \"\"\"1.Her hair was a tangled mess which she tried to make presentable by putting in a lump on the top of her head. 2.It didn't really work although it was a valiant attempt. While most people simply noticed the tangled mess on top of her head, what most people failed to understand that within the tangles mess was an entirely new year.3.That was her secret.4. She kept worlds on top of her head.\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "res = ''.join([i for i in ini_string if not i.isdigit()])\n",
    "\n",
    "print(\"final string : \", res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f24973ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.Her',\n",
       " 'hair',\n",
       " 'was',\n",
       " 'a',\n",
       " 'tangled',\n",
       " 'mess',\n",
       " 'which',\n",
       " 'she',\n",
       " 'tried',\n",
       " 'to',\n",
       " 'make',\n",
       " 'presentable',\n",
       " 'by',\n",
       " 'putting',\n",
       " 'in',\n",
       " 'a',\n",
       " 'lump',\n",
       " 'on',\n",
       " 'the',\n",
       " 'top',\n",
       " 'of',\n",
       " 'her',\n",
       " 'head',\n",
       " '.',\n",
       " '.It',\n",
       " 'did',\n",
       " \"n't\",\n",
       " 'really',\n",
       " 'work',\n",
       " 'although',\n",
       " 'it',\n",
       " 'was',\n",
       " 'a',\n",
       " 'valiant',\n",
       " 'attempt',\n",
       " '.',\n",
       " 'While',\n",
       " 'most',\n",
       " 'people',\n",
       " 'simply',\n",
       " 'noticed',\n",
       " 'the',\n",
       " 'tangled',\n",
       " 'mess',\n",
       " 'on',\n",
       " 'top',\n",
       " 'of',\n",
       " 'her',\n",
       " 'head',\n",
       " ',',\n",
       " 'what',\n",
       " 'most',\n",
       " 'people',\n",
       " 'failed',\n",
       " 'to',\n",
       " 'understand',\n",
       " 'that',\n",
       " 'within',\n",
       " 'the',\n",
       " 'tangles',\n",
       " 'mess',\n",
       " 'was',\n",
       " 'an',\n",
       " 'entirely',\n",
       " 'new',\n",
       " 'year',\n",
       " '..',\n",
       " 'That',\n",
       " 'was',\n",
       " 'her',\n",
       " 'secret',\n",
       " '..',\n",
       " 'She',\n",
       " 'kept',\n",
       " 'worlds',\n",
       " 'on',\n",
       " 'top',\n",
       " 'of',\n",
       " 'her',\n",
       " 'head',\n",
       " '.']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split the text into words\n",
    "word_tokens = word_tokenize(res)\n",
    "word_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84e3b6e",
   "metadata": {},
   "source": [
    "# Different Tokenizers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd22e18",
   "metadata": {},
   "source": [
    "## 1. WhiteSpace Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "303bb058",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1.Her', 'hair', 'was', 'a', 'tangled', 'mess', 'which', 'she', 'tried', 'to', 'make', 'presentable', 'by', 'putting', 'in', 'a', 'lump', 'on', 'the', 'top', 'of', 'her', 'head.', '2.It', \"didn't\", 'really', 'work', 'although', 'it', 'was', 'a', 'valiant', 'attempt.', 'While', 'most', 'people', 'simply', 'noticed', 'the', 'tangled', 'mess', 'on', 'top', 'of', 'her', 'head,', 'what', 'most', 'people', 'failed', 'to', 'understand', 'that', 'within', 'the', 'tangles', 'mess', 'was', 'an', 'entirely', 'new', 'year.3.That', 'was', 'her', 'secret.4.', 'She', 'kept', 'worlds', 'on', 'top', 'of', 'her', 'head.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "whitespace_tokenized = WhitespaceTokenizer().tokenize(txt)\n",
    "print(whitespace_tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b828aee",
   "metadata": {},
   "source": [
    "## 2. TreeBankWord Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7586e1f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1.Her', 'hair', 'was', 'a', 'tangled', 'mess', 'which', 'she', 'tried', 'to', 'make', 'presentable', 'by', 'putting', 'in', 'a', 'lump', 'on', 'the', 'top', 'of', 'her', 'head.', '2.It', 'did', \"n't\", 'really', 'work', 'although', 'it', 'was', 'a', 'valiant', 'attempt.', 'While', 'most', 'people', 'simply', 'noticed', 'the', 'tangled', 'mess', 'on', 'top', 'of', 'her', 'head', ',', 'what', 'most', 'people', 'failed', 'to', 'understand', 'that', 'within', 'the', 'tangles', 'mess', 'was', 'an', 'entirely', 'new', 'year.3.That', 'was', 'her', 'secret.4.', 'She', 'kept', 'worlds', 'on', 'top', 'of', 'her', 'head', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "treebank_tokenized = TreebankWordTokenizer().tokenize(txt)\n",
    "print(treebank_tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54ec1a2",
   "metadata": {},
   "source": [
    "## 3. MWE Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ff6b1739",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1', '.', 'H', 'e', 'r', ' ', 'h', 'a', 'i', 'r', ' ', 'w', 'a', 's', ' ', 'a', ' ', 't', 'a', 'n', 'g', 'l', 'e', 'd', ' ', 'm', 'e', 's', 's', ' ', 'w', 'h', 'i', 'c', 'h', ' ', 's', 'h_e', ' ', 't', 'r', 'i', 'e', 'd', ' ', 't', 'o', ' ', 'm', 'a', 'k', 'e', ' ', 'p', 'r', 'e', 's', 'e', 'n', 't', 'a', 'b', 'l', 'e', ' ', 'b', 'y', ' ', 'p', 'u', 't', 't', 'i', 'n', 'g', ' ', 'i', 'n', ' ', 'a', ' ', 'l', 'u', 'm', 'p', ' ', 'o', 'n', ' ', 't', 'h_e', ' ', 't', 'o', 'p', ' ', 'o', 'f', ' ', 'h_e', 'r', ' ', 'h_e', 'a', 'd', '.', ' ', '2', '.', 'I', 't', ' ', 'd', 'i', 'd', 'n', \"'\", 't', ' ', 'r', 'e', 'a', 'l', 'l', 'y', ' ', 'w', 'o', 'r', 'k', ' ', 'a', 'l', 't', 'h', 'o', 'u', 'g', 'h', ' ', 'i', 't', ' ', 'w', 'a', 's', ' ', 'a', ' ', 'v', 'a', 'l', 'i', 'a', 'n', 't', ' ', 'a', 't', 't', 'e', 'm', 'p', 't', '.', ' ', 'W', 'h', 'i', 'l', 'e', ' ', 'm', 'o', 's', 't', ' ', 'p', 'e', 'o', 'p', 'l', 'e', ' ', 's', 'i', 'm', 'p', 'l', 'y', ' ', 'n', 'o', 't', 'i', 'c', 'e', 'd', ' ', 't', 'h_e', ' ', 't', 'a', 'n', 'g', 'l', 'e', 'd', ' ', 'm', 'e', 's', 's', ' ', 'o', 'n', ' ', 't', 'o', 'p', ' ', 'o', 'f', ' ', 'h_e', 'r', ' ', 'h_e', 'a', 'd', ',', ' ', 'w', 'h', 'a', 't', ' ', 'm', 'o', 's', 't', ' ', 'p', 'e', 'o', 'p', 'l', 'e', ' ', 'f', 'a', 'i', 'l', 'e', 'd', ' ', 't', 'o', ' ', 'u', 'n', 'd', 'e', 'r', 's', 't', 'a', 'n', 'd', ' ', 't', 'h', 'a', 't', ' ', 'w', 'i', 't', 'h', 'i', 'n', ' ', 't', 'h_e', ' ', 't', 'a', 'n', 'g', 'l', 'e', 's', ' ', 'm', 'e', 's', 's', ' ', 'w', 'a', 's', ' ', 'a', 'n', ' ', 'e', 'n', 't', 'i', 'r', 'e', 'l', 'y', ' ', 'n', 'e', 'w', ' ', 'y', 'e', 'a', 'r', '.', '3', '.', 'T', 'h', 'a', 't', ' ', 'w', 'a', 's', ' ', 'h_e', 'r', ' ', 's', 'e', 'c', 'r', 'e', 't', '.', '4', '.', ' ', 'S', 'h_e', ' ', 'k', 'e', 'p', 't', ' ', 'w', 'o', 'r', 'l', 'd', 's', ' ', 'o', 'n', ' ', 't', 'o', 'p', ' ', 'o', 'f', ' ', 'h_e', 'r', ' ', 'h_e', 'a', 'd', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import MWETokenizer\n",
    "mwe_tokenizer = MWETokenizer([('he'),('take','a','good')])\n",
    "mwe_tokenizer.add_mwe(('Harry'))\n",
    "mwe_tokenized = mwe_tokenizer.tokenize(txt)\n",
    "print(mwe_tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aee9d7b",
   "metadata": {},
   "source": [
    "## 4. Tweet Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7e300f81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1.Her', 'hair', 'was', 'a', 'tangled', 'mess', 'which', 'she', 'tried', 'to', 'make', 'presentable', 'by', 'putting', 'in', 'a', 'lump', 'on', 'the', 'top', 'of', 'her', 'head', '.', '2.It', \"didn't\", 'really', 'work', 'although', 'it', 'was', 'a', 'valiant', 'attempt', '.', 'While', 'most', 'people', 'simply', 'noticed', 'the', 'tangled', 'mess', 'on', 'top', 'of', 'her', 'head', ',', 'what', 'most', 'people', 'failed', 'to', 'understand', 'that', 'within', 'the', 'tangles', 'mess', 'was', 'an', 'entirely', 'new', 'year.3.That', 'was', 'her', 'secret', '.', '4', '.', 'She', 'kept', 'worlds', 'on', 'top', 'of', 'her', 'head', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "tweet_tokenized = TweetTokenizer().tokenize(txt)\n",
    "print(tweet_tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348417ee",
   "metadata": {},
   "source": [
    "## 5. Punctuation Based Word Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "476df157",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1', '.', 'Her', 'hair', 'was', 'a', 'tangled', 'mess', 'which', 'she', 'tried', 'to', 'make', 'presentable', 'by', 'putting', 'in', 'a', 'lump', 'on', 'the', 'top', 'of', 'her', 'head', '.', '2', '.', 'It', 'didn', \"'\", 't', 'really', 'work', 'although', 'it', 'was', 'a', 'valiant', 'attempt', '.', 'While', 'most', 'people', 'simply', 'noticed', 'the', 'tangled', 'mess', 'on', 'top', 'of', 'her', 'head', ',', 'what', 'most', 'people', 'failed', 'to', 'understand', 'that', 'within', 'the', 'tangles', 'mess', 'was', 'an', 'entirely', 'new', 'year', '.', '3', '.', 'That', 'was', 'her', 'secret', '.', '4', '.', 'She', 'kept', 'worlds', 'on', 'top', 'of', 'her', 'head', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize\n",
    "print(wordpunct_tokenize(txt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ffbf24",
   "metadata": {},
   "source": [
    "## 6. Punctuation Based Sentence Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aeb6e14a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1.Her hair was a tangled mess which she tried to make presentable by putting in a lump on the top of her head.', \"2.It didn't really work although it was a valiant attempt.\", 'While most people simply noticed the tangled mess on top of her head, what most people failed to understand that within the tangles mess was an entirely new year.3.That was her secret.4.', 'She kept worlds on top of her head.']\n"
     ]
    }
   ],
   "source": [
    "sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "sent_tokenized = sent_detector.tokenize(txt)\n",
    "print(sent_tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e129525f",
   "metadata": {},
   "source": [
    "# Different Stemming Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f300f8e",
   "metadata": {},
   "source": [
    "## 1. SnowBall Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9fd64f3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Snowball Stemming\n",
      "['1.Her', 'hair', 'was', 'a', 'tangled', 'mess', 'which', 'she', 'tried', 'to', 'make', 'presentable', 'by', 'putting', 'in', 'a', 'lump', 'on', 'the', 'top', 'of', 'her', 'head', '.', '2.It', \"didn't\", 'really', 'work', 'although', 'it', 'was', 'a', 'valiant', 'attempt', '.', 'While', 'most', 'people', 'simply', 'noticed', 'the', 'tangled', 'mess', 'on', 'top', 'of', 'her', 'head', ',', 'what', 'most', 'people', 'failed', 'to', 'understand', 'that', 'within', 'the', 'tangles', 'mess', 'was', 'an', 'entirely', 'new', 'year.3.That', 'was', 'her', 'secret', '.', '4', '.', 'She', 'kept', 'worlds', 'on', 'top', 'of', 'her', 'head', '.'] \n",
      "\n",
      "After Snowball Stemming\n",
      "['1.her', 'hair', 'was', 'a', 'tangl', 'mess', 'which', 'she', 'tri', 'to', 'make', 'present', 'by', 'put', 'in', 'a', 'lump', 'on', 'the', 'top', 'of', 'her', 'head', '.', '2.it', \"didn't\", 'realli', 'work', 'although', 'it', 'was', 'a', 'valiant', 'attempt', '.', 'while', 'most', 'peopl', 'simpli', 'notic', 'the', 'tangl', 'mess', 'on', 'top', 'of', 'her', 'head', ',', 'what', 'most', 'peopl', 'fail', 'to', 'understand', 'that', 'within', 'the', 'tangl', 'mess', 'was', 'an', 'entir', 'new', 'year.3.that', 'was', 'her', 'secret', '.', '4', '.', 'she', 'kept', 'world', 'on', 'top', 'of', 'her', 'head', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "snowballstemmer = SnowballStemmer(language='english')\n",
    "stem_words = []\n",
    "for w in tweet_tokenized:\n",
    "    x = snowballstemmer.stem(w)\n",
    "    stem_words.append(x)\n",
    "print('Before Snowball Stemming')\n",
    "print(tweet_tokenized,'\\n')\n",
    "\n",
    "print('After Snowball Stemming')\n",
    "print(stem_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac04d431",
   "metadata": {},
   "source": [
    "## 2. Porter Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "68abe658",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Porter Stemming\n",
      "['1.Her', 'hair', 'was', 'a', 'tangled', 'mess', 'which', 'she', 'tried', 'to', 'make', 'presentable', 'by', 'putting', 'in', 'a', 'lump', 'on', 'the', 'top', 'of', 'her', 'head', '.', '2.It', \"didn't\", 'really', 'work', 'although', 'it', 'was', 'a', 'valiant', 'attempt', '.', 'While', 'most', 'people', 'simply', 'noticed', 'the', 'tangled', 'mess', 'on', 'top', 'of', 'her', 'head', ',', 'what', 'most', 'people', 'failed', 'to', 'understand', 'that', 'within', 'the', 'tangles', 'mess', 'was', 'an', 'entirely', 'new', 'year.3.That', 'was', 'her', 'secret', '.', '4', '.', 'She', 'kept', 'worlds', 'on', 'top', 'of', 'her', 'head', '.'] \n",
      "\n",
      "After Porter Stemming\n",
      "['1.her', 'hair', 'wa', 'a', 'tangl', 'mess', 'which', 'she', 'tri', 'to', 'make', 'present', 'by', 'put', 'in', 'a', 'lump', 'on', 'the', 'top', 'of', 'her', 'head', '.', '2.it', \"didn't\", 'realli', 'work', 'although', 'it', 'wa', 'a', 'valiant', 'attempt', '.', 'while', 'most', 'peopl', 'simpli', 'notic', 'the', 'tangl', 'mess', 'on', 'top', 'of', 'her', 'head', ',', 'what', 'most', 'peopl', 'fail', 'to', 'understand', 'that', 'within', 'the', 'tangl', 'mess', 'wa', 'an', 'entir', 'new', 'year.3.that', 'wa', 'her', 'secret', '.', '4', '.', 'she', 'kept', 'world', 'on', 'top', 'of', 'her', 'head', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "porterstemmer = PorterStemmer()\n",
    "stem_words = []\n",
    "for w in tweet_tokenized:\n",
    "    x = porterstemmer.stem(w)\n",
    "    stem_words.append(x)\n",
    "print('Before Porter Stemming')\n",
    "print(tweet_tokenized,'\\n')\n",
    "\n",
    "print('After Porter Stemming')\n",
    "print(stem_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0598a0ba",
   "metadata": {},
   "source": [
    "# Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2b8d95e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Lemmatization\n",
      "['1.Her', 'hair', 'was', 'a', 'tangled', 'mess', 'which', 'she', 'tried', 'to', 'make', 'presentable', 'by', 'putting', 'in', 'a', 'lump', 'on', 'the', 'top', 'of', 'her', 'head', '.', '2.It', \"didn't\", 'really', 'work', 'although', 'it', 'was', 'a', 'valiant', 'attempt', '.', 'While', 'most', 'people', 'simply', 'noticed', 'the', 'tangled', 'mess', 'on', 'top', 'of', 'her', 'head', ',', 'what', 'most', 'people', 'failed', 'to', 'understand', 'that', 'within', 'the', 'tangles', 'mess', 'was', 'an', 'entirely', 'new', 'year.3.That', 'was', 'her', 'secret', '.', '4', '.', 'She', 'kept', 'worlds', 'on', 'top', 'of', 'her', 'head', '.'] \n",
      "\n",
      "After Lemmatization\n",
      "['1.Her', 'hair', 'wa', 'a', 'tangled', 'mess', 'which', 'she', 'tried', 'to', 'make', 'presentable', 'by', 'putting', 'in', 'a', 'lump', 'on', 'the', 'top', 'of', 'her', 'head', '.', '2.It', \"didn't\", 'really', 'work', 'although', 'it', 'wa', 'a', 'valiant', 'attempt', '.', 'While', 'most', 'people', 'simply', 'noticed', 'the', 'tangled', 'mess', 'on', 'top', 'of', 'her', 'head', ',', 'what', 'most', 'people', 'failed', 'to', 'understand', 'that', 'within', 'the', 'tangle', 'mess', 'wa', 'an', 'entirely', 'new', 'year.3.That', 'wa', 'her', 'secret', '.', '4', '.', 'She', 'kept', 'world', 'on', 'top', 'of', 'her', 'head', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "stem_words = []\n",
    "for w in tweet_tokenized:\n",
    "    x = wordnet_lemmatizer.lemmatize(w)\n",
    "    stem_words.append(x)\n",
    "print('Before Lemmatization')\n",
    "print(tweet_tokenized,'\\n')\n",
    "\n",
    "print('After Lemmatization')\n",
    "print(stem_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f9dd6c",
   "metadata": {},
   "source": [
    "#  Text Without Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fe9dfbee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1Her hair was a tangled mess which she tried to make presentable by putting in a lump on the top of her head 2It didnt really work although it was a valiant attempt While most people simply noticed the tangled mess on top of her head what most people failed to understand that within the tangles mess was an entirely new year3That was her secret4 She kept worlds on top of her head\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "res1 = re.sub(r'[^\\w\\s]', '', txt)\n",
    "\n",
    "# printing result\n",
    "print( res1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c3b8a0",
   "metadata": {},
   "source": [
    "# Make Uppercase letters to Lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7f3bf918",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting uppercase to lowercase we can use .lower() function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "477750df",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = \"\"\"1.Her hair was a tangled mess which she tried to make presentable by putting in a lump on the top of her head. 2.It didn't really work although it was a valiant attempt. While most people simply noticed the tangled mess on top of her head, what most people failed to understand that within the tangles mess was an entirely new year.3.That was her secret.4. She kept worlds on top of her head.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b514befd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.her hair was a tangled mess which she tried to make presentable by putting in a lump on the top of her head. 2.it didn't really work although it was a valiant attempt. while most people simply noticed the tangled mess on top of her head, what most people failed to understand that within the tangles mess was an entirely new year.3.that was her secret.4. she kept worlds on top of her head.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Convert uppercase letters to lowercase\n",
    "lowercase_text = txt.lower()\n",
    "\n",
    "print(lowercase_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b717c916",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.Herhairwasatangledmesswhichshetriedtomakepresentablebyputtinginalumponthetopofherhead.2.Itdidn'treallyworkalthoughitwasavaliantattempt.Whilemostpeoplesimplynoticedthetangledmessontopofherhead,whatmostpeoplefailedtounderstandthatwithinthetanglesmesswasanentirelynewyear.3.Thatwashersecret.4.Shekeptworldsontopofherhead.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "txt = \"\"\"1.Her hair was a tangled mess which she tried to make presentable by putting in a lump on the top of her head. 2.It didn't really work although it was a valiant attempt. While most people simply noticed the tangled mess on top of her head, what most people failed to understand that within the tangles mess was an entirely new year.3.That was her secret.4. She kept worlds on top of her head.\"\"\"\n",
    "\n",
    "# Remove all whitespaces using regular expressions\n",
    "text_without_spaces = re.sub(r'\\s', '', txt)\n",
    "\n",
    "print(text_without_spaces)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44205e5",
   "metadata": {},
   "source": [
    "# Use of Inflect Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "49161f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import docx\n",
    "import inflect\n",
    "\n",
    "doc = docx.Document('sample.docx')\n",
    "\n",
    "# instance for inflect engine\n",
    "p = inflect.engine()\n",
    "\n",
    "for paragraph in doc.paragraphs:\n",
    "    # Access to text of paragraph\n",
    "    text = paragraph.text\n",
    "\n",
    "    # Split the text into words\n",
    "    words = text.split()\n",
    "\n",
    "    # loop to iterate through the words to find the numbers\n",
    "    for i, word in enumerate(words):\n",
    "        if word.isdigit():\n",
    "            # Convert the number to words\n",
    "            words[i] = p.number_to_words(word)\n",
    "\n",
    "    # Reconstruct the modified text\n",
    "    modified_text = ' '.join(words)\n",
    "\n",
    "    # Update the paragraph with the modified text\n",
    "    paragraph.text = modified_text\n",
    "\n",
    "# Save the modified document\n",
    "doc.save('modified_document.docx')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0993263a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
