{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c798469e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of lines in file is:  3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[\"Hello my name is yashraj . \\\\n',\\n\",\n",
       " \" 'I am honest and handsome . \\\\n',\\n\",\n",
       " \" 'I like swimmig ,\\\\\\\\ dancing , browsing internet .\"]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#  open the text file\n",
    "file = open('File.txt', encoding='utf8')\n",
    "read = file.read()\n",
    "file.seek(0)\n",
    "read\n",
    " \n",
    "# get number of lines in file and print it\n",
    "line = 1\n",
    "for word in read:\n",
    "    if word == '\\n':\n",
    "        line += 1\n",
    "print(\"Number of lines in file is: \", line)\n",
    " \n",
    "# create a list to store each line as an element of list\n",
    "\n",
    "array = []\n",
    "for i in range(line):\n",
    "    array.append(file.readline())\n",
    " \n",
    "array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2fbfecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "punc = '''!()-[]{};:'\"\\, <>./?@#$%^&*_~'''\n",
    "for ele in read:  \n",
    "    if ele in punc:  \n",
    "        read = read.replace(ele, \" \")  \n",
    "         \n",
    "read\n",
    " \n",
    "# to maintain uniformity\n",
    "read=read.lower()                    \n",
    "read\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643cb005",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d350ae62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_words(file_contents):\n",
    "    \"\"\"\n",
    "    Tokenizes the file contents.\n",
    "     \n",
    "    Parameters\n",
    "    ----------\n",
    "    file_contents : list\n",
    "        A list of strings containing the contents of the file.\n",
    "     \n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        A list of strings containing the contents of the file tokenized.\n",
    "     \n",
    "    \"\"\"\n",
    "    result = []\n",
    " \n",
    "    for i in range(len(file_contents)):\n",
    "        tokenized = []\n",
    " \n",
    "        # print(\"The row is \", file_contents[i])\n",
    " \n",
    "        # split the line by spaces\n",
    "        tokenized = file_contents[i].split()\n",
    " \n",
    "        result.append(tokenized)\n",
    " \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58dc802a",
   "metadata": {},
   "source": [
    "## Clean data by removing stopwords:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9e77338",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/yashraj/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'yashraj', '.', '\\\\n', \"'\", ',', \"'\", 'I', 'honest', 'handsome', '.', '\\\\n', \"'\", ',', \"'\", 'I', 'swimmig', ',', '\\\\\\\\', 'dancing', ',', 'browsing', 'internet', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    " \n",
    "for i in range(1):\n",
    "    # this will convert\n",
    "    # the word into tokens\n",
    "    text_tokens = word_tokenize(read)\n",
    " \n",
    "tokens_without_sw = [\n",
    "    word for word in text_tokens if not word in stopwords.words()]\n",
    " \n",
    "print(tokens_without_sw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635f7bdf",
   "metadata": {},
   "source": [
    "## Create an inverted index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "efa0f4c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'yashraj': [1],\n",
       " '.': [1, 1, 1, 2, 2, 2, 3, 3, 3],\n",
       " '\\\\n': [1, 1, 2, 2],\n",
       " \"'\": [1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3],\n",
       " ',': [1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3],\n",
       " 'honest': [2],\n",
       " 'handsome': [2],\n",
       " 'swimmig': [3],\n",
       " '\\\\\\\\': [3],\n",
       " 'dancing': [3],\n",
       " 'browsing': [3],\n",
       " 'internet': [3]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict = {}\n",
    " \n",
    "for i in range(line):\n",
    "    check = array[i].lower()\n",
    "    for item in tokens_without_sw:\n",
    " \n",
    "        if item in check:\n",
    "            if item not in dict:\n",
    "                dict[item] = []\n",
    " \n",
    "            if item in dict:\n",
    "                dict[item].append(i+1)\n",
    " \n",
    "dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444dfd5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62004c32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab34f801",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c874ee57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fox -> Document 1\n",
      "in -> Document 2\n",
      "quick -> Document 1\n",
      "over -> Document 1\n",
      "the -> Document 1, Document 2\n",
      "sun. -> Document 2\n",
      "dog. -> Document 1\n",
      "slept -> Document 2\n",
      "dog -> Document 2\n",
      "brown -> Document 1\n",
      "lazy -> Document 1, Document 2\n",
      "jumped -> Document 1\n"
     ]
    }
   ],
   "source": [
    "# Define the documents\n",
    "document1 = \"The quick brown fox jumped over the lazy dog.\"\n",
    "document2 = \"The lazy dog slept in the sun.\"\n",
    "\n",
    "# Step 1: Tokenize the documents\n",
    "# Convert each document to lowercase and split it into words\n",
    "tokens1 = document1.lower().split()\n",
    "tokens2 = document2.lower().split()\n",
    "\n",
    "# Combine the tokens into a list of unique terms\n",
    "terms = list(set(tokens1 + tokens2))\n",
    "\n",
    "# Step 2: Build the inverted index\n",
    "# Create an empty dictionary to store the inverted index\n",
    "inverted_index = {}\n",
    "\n",
    "# For each term, find the documents that contain it\n",
    "for term in terms:\n",
    "\tdocuments = []\n",
    "\tif term in tokens1:\n",
    "\t\tdocuments.append(\"Document 1\")\n",
    "\tif term in tokens2:\n",
    "\t\tdocuments.append(\"Document 2\")\n",
    "\tinverted_index[term] = documents\n",
    "\n",
    "# Step 3: Print the inverted index\n",
    "for term, documents in inverted_index.items():\n",
    "\tprint(term, \"->\", \", \".join(documents))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d02796",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebff3919",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e39e5d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
